\section{Реализация системы}
\label{sec:Chapter5} \index{Chapter5}

\subsection{Используемые технологии}

Архитектура, описанная в главе [\ref{sec:Chapter4}], подразумевает реализацию сервиса координатора и отдельных проверяющих модулей.
Это должны быть программы-сервисы, которые можно один раз запустить и ожидать, что они будут постоянно работать, обслуживая вдодящие сообщения.
В качестве языка программирования для этих сервисов был выбран Python 3. 
Такой выбор обусловлен широким распространением языка в академической среде.
Код, написанный на Python, легко читать и понимать, легко вносить правки.
Кроме того у языка большое сообщество пользователей, существует огромное количество библиотек для работы с различными внешними технологиями.
Последний пункт про библиотеки крайне важен в рамках данной работы, так как архитектурная схема подразумевает использование 
внешней реляционной базы данных и брокера сообщений. 

На сегодняшний день существует большое количество различных реляционных систем управления базами данных (РСУБД).
Согласно \cite{wikiRDBMS} их число на данный момент приближается к ста.
Тем не менее между ними существуют отличия, опираясь на которые и нужно делать выбор.
Важно отметить, что данный проект носит исключительно некоммерческий характер, поэтому в нем должны быть использованы только 
свободно распростроняемые технологии с открытым исходным кодом.
Также при выборе стоит учитывать наличие и качество библиотек для работы с базой для того языкы программирования, который используется в проекте.
Наиболее популярной и распространенной РСУБД с открытым исходным кодом на данный момент является PostgreSQL.
Именно она используется в качестве базы данных в данном проекте.

Выбор брокеров сообщений менее обширный.
Наибольшей популярностью на сегодняшний день пользуется Apache Kafka, однако для целей нашего проекта ее функционал и возможности будут излишни.
За это придется заплатить легкостью администирования и скоростью настройки.
Поэтому выбор пал на RabbitMQ.
Это брокер сообщений с открытым исходными кодом, реализующий функционал очередей, и дающий гарантии относительно целостности и долговечности данных.
<<Из коробки>> предоставляется панель администратора, в которой вручную можно управлять очередями, а также отслеживать ключевые метрики системы.
Также большим плюсом является наличие асинхронного клиента на языке Python, который позволяет обрабатывать входящие сообщения независимо 
с использованием корутин, которые появились в языке начиная с версии 3.4.

\subsection{Реализованные сервисы}

В рамках данной работы были реализованы два сервиса проверки. 
Первый - ejudge-checker - должен осуществлять проверку с помощью Ejudge.
Второй - mpi-checker -  должен самостоятельно проверять MPI задачи.
Также было реализовано <<сердце>> системы - сервис coordinator.

Сервис coordinator решает три основные задачи.
Во-первых, он реализует REST API для отправки решения на проверку и получения актуального статуса проверки.
Для этого был использован фреймворк django \cite{django}, позволяющий быстро поднять HTTP сервер и удобно настроить маршрутизацию входящих запросов.
Во-вторых, он отвечает за хранение состояния проверок, и гарантирует целостность и долговечность этого состояния.
Для этого он использует реляционную базу данных PostgreSQL.
Каждой проверке присваивается уникальный идентификатор и создается соотвествующая запись в таблице.
В-третьих, сервис координирует работу проверяющих модулей.
На основе полученной от LMS информации он формирует архив со всеми необходимым для проверки файлами.
Затем отправляет полученный архив в очередь, соответствующую нужному проверяющему модулю.
Одновременно с этим он постоянно слушает обновления из очереди результатов проверок.

Сервисы ejudge-checker и mpi-checker являются проверяющими модулями.
Они похожи с точки зрения интерфейса.
Оба сервисы подписываются на очередь сообщений и вычитывают сообщения по соотвествующему ключу.
Далее они осуществляют проверки и формирует сообщение с результатом проверки, которое отсылают в другую очередь.
Однако два сервиса совершенно по-разному реализуют описанный интерфейс.

Ejudge-checker устроен достаточно просто.
Он принимает архив, достает из него файл с решением, получает информацию о том, к какой задаче в терминах Ejudge это решение относится 
и с использованием интерфейса командной строки отправляет решение на проверку в Ejudge.
Затем он периодически опрашивает Ejudge об актуальном статусе проверки, а дождавшись результата, формирует сообщение с результатом для координатора.

MPI-checker устроен чуть более сложно.
Модуль проверки умеет компилировать, запускать и сравнивать выходные данные MPI программ с ожидаемыми.
При его реализации было поддержано введение дополнительных ограничений на решения.
У администратора системы есть возможность выставить ограничение на решение по времени исполнения и по используемой памяти.
Также есть возможность ограничить набор используемых MPI функций и число их использований.
Сравнение реальных выходных данных и ожидаемых производится отдельно для каждого процесса.

Для реализации требования на ограничение по времени и памяти был использован менеджер нагрузки Slurm \cite{slurm}.
Он позволяет управлять распределенными ресурсами, аллоцировать их на выполнение задач, и запускать задачи с использованием выделенных ресурсов.
Slurm поддерживает очередь выполнения задач.
Как только в системе есть достаточно свободных ресурсов для выполнения следующей задачи из очереди, он приступает к ней.
Взаимодействие со Slurm чем-то похоже на взаимодействие с Ejudge. 
Сначала задача посылается на выполнение, а затем необходимо отслеживать ее статус и ждать, пока Slurm ее выполнит.
<<Из коробки>> Slurm позволяет настроить для каждой задачи отдельные лимиты по времени исполнения и используемой памяти.
Чуть более интересно ситуация обстоит с тем, как ограничивается набор используемых MPI- функций, о чем речь пойдет в следующей подглаве.

\subsection{MPI профайлер}

Для реализации ограничения на MPI функции необходимо отслеживать, какие функции используются в решении студента.
Наивный подход мог бы заключаться в анализе исходного кода программ для выявления явного использования запрещенных функций.
Однако такой подход не позволяет задать ограничение на количество использований определенной функции.
Например, мы хотим дать студенту возможность использовать конкретную функцию не более двух раз.
Наличие вызова функции в коде программы не противоречит данному ограничению, однако неясно, 
сколько раз функция будет вызвана, если вызов происходит, например, в цикле.
Поэтому единственным способом реализовать данное ограничение будет профилирование программы.

Спецификация Open MPI \cite{mpiSpec} содержит информацию о профилировании.
Любая реализация MPI должна предоставлять интерфейс, который позволял бы разрабатывать сторонние дебагеры, 
анализаторы производительности и другие инструменты, использующие информацию о ходе исполнения программ.
Такой интерфейс называется интерфейсом профилирования.
Благодаря тому, что он является частью спецификации, гарантируется, что профайлер или дебагер, 
написанный и протестированный с конкретной реализацией, будет также работать с любой другой реализацией.
Согласно спецификации такой интерфейс должен предоставляться следующим образом.
Все функции, декларируемые в спецификации MPI, должны быть реализованы и иметь две альтернативных точки входа.
Основная функция, декларируемая спецификацией, должна иметь префикс <<MPI\_>>, однако у нее должна существовать пара с префиксом <<PMPI\_>>, 
которая должна быть идентична оригинальной функции.
Это позволяет разработать динамическую библиотеку, в которой будут переопределяться функции с профексом <<MPI\_>>.
При этом новые переопределенные функции внутри себя будут вызывать функции с префиксом <PMPI\_>>.
Вокруг этого вызова могут совершаться любые операции, которые нужны разработчикам профайлеров.
Засчет того, что библиотека динамическая, для подключения профайлера не требуется перекомпиляция оригинальной программы.
Достаточно заново произвести данамическую линковку.
Такой подход очевидно приводит к появлению накладных расходов. 
И расходы тем выше, чем сложнее логика, реализованная на слое профилирования.

В открытом доступе был найден только один готовый MPI профайлер с открытым исходным кодом - mpiP \cite{mpiP}.
При попытке использовать его для решения нашей задачи выяснилось, что формат вывода результатов профилирования плохо структурирован и
было бы достаточно сложно парсить его программным способом.
К тому же количество функций и вомзможностей профайлера является избыточном для нашей задачи, 
а значит мы получим накладные расхода на то, что нам не требуется.
Поэтому было принято решение реализовать свой собственной профайлер.

Для решения нашей задачи требуется только вести счетчик количества вызовов MPI функций.
С точки зрения реализации это делается следующим образом.
Для каждой функции из спецификации MPI надо написать обертку, которая будет регистировать вызов соотвествующей MPI функции, 
а затем вызывать оригинальную функцию с теми же аргументами.
Рассмотрим на примере обертки для функции MPI\_Send.

\pagebreak
\begin{lstlisting}[language=C, breaklines=true]

MPI_CALLS = malloc(sizeof(int) * MPI_FUNCTIONS_COUNT);
for (int i = 0; i < MPI_FUNCTIONS_COUNT; i++) {
    MPI_CALLS[i] = 0;
}
...
void REGISTER_CALL(int funcId) {
  MPI_CALLS[funcId]++;
}
...
extern int MPI_Send(const void *buf, int count, MPI_Datatype datatype,  int dest, int tag, MPI_Comm comm) {
  REGISTER_CALL(108);
  int res = PMPI_Send(buf, count, datatype, dest, tag, comm);
  return res;
}
\end{lstlisting}

В коде, представленном выше, число 108 это порядковый номер функции MPI\_Send в спецификации.
Стоит отметить, что на данный момент в спецификации MPI задекларировано 146 функций, и написание вручную обертки для каждой из них 
заняло много времени и привело бы к большому количеству багов.
Поэтому этот процесс необходимо было автоматизировать и написать алгоритм кодогенерации.
В итоге был разработан python скрипт, который на основе спецификации генерирует код бибиотеки профайлера на языке C.
Исходный код профайлера доступен в Github репозитории \cite{mpiProfilerGithub}.

Последнее требование к проверяющему сервису - возможность независимо оценивать выходные потоки разных MPI процессов.
Утилита mpirun, используемая для запуска MPI программ, создает один общий поток вывода и пишет в него все сообщения, получаемые от разных процессов.
Нет возможности каким-либо образом без изменений кода программы разделить потоки вывода.
Важно отметить, что в ходе реализации предыдущего требования уже был разработан собственный профайлер, который будет использоваться для всех запусков MPI программ.
Таким образом мы можем интегрировать дополнительную логику во все проверяемые решения, так как 
в коде профайлера у нас есть возможность добавлять эту логику в слое интерфейса профилирования.
В том числе в разработанном профайлере уже есть переопределение функции MPI\_INIT, 
которая вызывается в любой MPI программе гарантированно раньше вызова всех остальных функций.
Это то место, где есть возможность переопределить поток вывода и сделать это независимо для каждого процесса.
Таким образом можно для каждого процесса настроить перенаправления потока вывода в отдельный файл.

\begin{lstlisting}[language=C, breaklines=true]
static int world_rank = 0;
static int world_size = 0;
...
void REDIRECT_STDOUT() {
    char* fn = (char*)malloc(15);
    sprintf(fn, "stdout_%03d.log", world_rank);
    freopen(fn, "w", stdout);
    free(fn);
}
...
extern int MPI_Init(int *argc, char ***argv) {
	int res = PMPI_Init(argc, argv);
	PMPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
    PMPI_Comm_size(MPI_COMM_WORLD, &world_size);
	REDIRECT_STDOUT();
	REGISTER_CALL(82);
	return res;
}
\end{lstlisting}

После запуска MPI программы с использованием 3 процессов (mpirun -n 3 a.out) получим в текущей дериктории 3 файла с именами stdout\_001.log, stdout\_002.log, stdout\_003.log.

\subsection{Пример работы системы (потенциально +3 страницы)} 