\section{Реализация системы}
\label{sec:Chapter5} \index{Chapter5}

\subsection{Используемые технологии}

Архитектура, описанная в главе [\ref{sec:Chapter4}], подразумевает реализацию сервиса координатора и отдельных проверяющих модулей.
Это должны быть программы-сервисы, которые можно один раз запустить и ожидать, что они будут постоянно работать, обслуживая входящие сообщения.
В качестве языка программирования для этих сервисов был выбран Python 3. 
Такой выбор обусловлен широким распространением языка в академической среде.
Код, написанный на Python, легко читать и понимать, легко вносить правки.
Кроме того у языка большое сообщество пользователей, существует огромное количество библиотек для работы с различными внешними технологиями.
Последний пункт про библиотеки крайне важен в рамках данной работы, так как архитектурная схема подразумевает использование 
внешней реляционной базы данных и брокера сообщений. 

На сегодняшний день существует большое количество различных реляционных систем управления базами данных (РСУБД).
Согласно \cite{wikiRDBMS} их число на данный момент приближается к ста.
Тем не менее между ними существуют отличия, опираясь на которые и нужно делать выбор.
Важно отметить, что данный проект носит исключительно некоммерческий характер, поэтому в нем должны быть использованы только 
свободно распространяемые технологии с открытым исходным кодом.
Также при выборе стоит учитывать наличие и качество библиотек для работы с базой для того языка программирования, который используется в проекте.
Наиболее популярной и распространенной РСУБД с открытым исходным кодом на данный момент является PostgreSQL.
Именно она используется в качестве базы данных в данном проекте.

Выбор брокеров сообщений менее обширный.
Наибольшей популярностью на сегодняшний день пользуется Apache Kafka, однако для целей нашего проекта ее функционал и возможности будут излишни.
За это придется заплатить легкостью администрирования и скоростью настройки.
Поэтому выбор пал на RabbitMQ.
Это брокер сообщений с открытым исходными кодом, реализующий функционал очередей, и дающий гарантии относительно целостности и долговечности данных.
<<Из коробки>> предоставляется панель администратора, в которой вручную можно управлять очередями, а также отслеживать ключевые метрики системы.
Также большим плюсом является наличие асинхронного клиента на языке Python, который позволяет обрабатывать входящие сообщения независимо 
с использованием корутин, которые появились в языке начиная с версии 3.4.

\subsection{Реализованные сервисы}

В рамках данной работы были реализованы два сервиса проверки. 
Первый - \codebox{ejudge-checker} - должен осуществлять проверку с помощью Ejudge.
Второй - \codebox{mpi-checker} -  должен самостоятельно проверять MPI задачи.
Также было реализовано <<сердце>> системы - сервис \codebox{coordinator}.

Сервис \codebox{coordinator} решает три основные задачи.
Во-первых, он реализует REST API для отправки решения на проверку и получения актуального статуса проверки.
Для этого был использован фреймворк Django \cite{django}, позволяющий быстро поднять HTTP сервер и удобно настроить маршрутизацию входящих запросов.
Во-вторых, он отвечает за хранение состояния проверок, и гарантирует целостность и долговечность этого состояния.
Для этого он использует реляционную базу данных PostgreSQL.
Каждой проверке присваивается уникальный идентификатор и создается соответствующая запись в таблице.
В-третьих, сервис координирует работу проверяющих модулей.
На основе полученной от LMS информации он формирует архив со всеми необходимым для проверки файлами.
Затем отправляет полученный архив в очередь, соответствующую нужному проверяющему модулю.
Одновременно с этим он постоянно слушает обновления из очереди результатов проверок.

Сервисы \codebox{ejudge-checker} и \codebox{mpi-checker} являются проверяющими модулями.
Они похожи с точки зрения интерфейса.
Оба сервисы подписываются на очередь сообщений и вычитывают сообщения по соответствующему ключу.
Далее они осуществляют проверки и формирует сообщение с результатом проверки, которое отсылают в другую очередь.
Однако два сервиса совершенно по-разному реализуют описанный интерфейс.

\codebox{Ejudge-checker} устроен достаточно просто.
Он принимает архив, достает из него файл с решением, получает информацию о том, к какой задаче в терминах Ejudge это решение относится 
и с использованием интерфейса командной строки отправляет решение на проверку в Ejudge.
Затем он периодически опрашивает Ejudge об актуальном статусе проверки, а дождавшись результата, формирует сообщение с результатом для координатора.

\codebox{MPI-checker} устроен чуть более сложно.
Модуль проверки умеет компилировать, запускать и сравнивать выходные данные MPI программ с ожидаемыми.
При его реализации было поддержано введение дополнительных ограничений на решения.
У администратора системы есть возможность выставить ограничение на решение по времени исполнения и по используемой памяти.
Также есть возможность ограничить набор используемых MPI функций и число их использований.
Сравнение реальных выходных данных и ожидаемых производится отдельно для каждого процесса.

Для реализации требования на ограничение по времени и памяти был использован менеджер нагрузки Slurm \cite{slurm}.
Он позволяет управлять распределенными ресурсами, аллоцировать их на выполнение задач, и запускать задачи с использованием выделенных ресурсов.
Slurm поддерживает очередь выполнения задач.
Как только в системе есть достаточно свободных ресурсов для выполнения следующей задачи из очереди, он приступает к ней.
Взаимодействие со Slurm чем-то похоже на взаимодействие с Ejudge. 
Сначала задача посылается на выполнение, а затем необходимо отслеживать ее статус и ждать, пока Slurm ее выполнит.
<<Из коробки>> Slurm позволяет настроить для каждой задачи отдельные лимиты по времени исполнения и используемой памяти.

Все реализованные сервисы доступны в публичном Bitbucket репозитории.\footnote{https://bitbucket.org/mipt-mpichecker/}

\subsection{MPI профайлер}

Для реализации ограничения на MPI функции необходимо отслеживать, какие функции используются в решении студента.
Наивный подход мог бы заключаться в анализе исходного кода программ для выявления явного использования запрещенных функций.
Однако такой подход не позволяет задать ограничение на количество использований определенной функции.
Например, мы хотим дать студенту возможность использовать конкретную функцию не более двух раз.
Наличие вызова функции в коде программы не противоречит данному ограничению, однако неясно, 
сколько раз функция будет вызвана, если вызов происходит, например, в цикле.
Поэтому единственным способом реализовать данное ограничение будет профилирование программы.

Спецификация Open MPI \cite{mpiSpec} содержит информацию о профилировании.
Любая реализация MPI должна предоставлять интерфейс, который позволял бы разрабатывать сторонние дебагеры, 
анализаторы производительности и другие инструменты, использующие информацию о ходе исполнения программ.
Такой интерфейс называется интерфейсом профилирования.
Благодаря тому, что он является частью спецификации, гарантируется, что профайлер или дебагер, 
написанный и протестированный с конкретной реализацией, будет также работать с любой другой реализацией.
Согласно спецификации такой интерфейс должен предоставляться следующим образом.
Все функции, декларируемые в спецификации MPI, должны быть реализованы и иметь две альтернативных точки входа.
Основная функция, декларируемая спецификацией, должна иметь префикс \codebox{MPI_}, однако у нее должна существовать пара с префиксом \codebox{PMPI_} 
которая должна быть идентична оригинальной функции.
Это позволяет разработать динамическую библиотеку, в которой будут переопределяться функции с префиксом \codebox{MPI_}.
При этом новые переопределенные функции внутри себя будут вызывать функции с префиксом \codebox{PMPI_}.
Вокруг этого вызова могут совершаться любые операции, которые нужны разработчикам профайлеров.
За счет того, что библиотека динамическая, для подключения профайлера не требуется перекомпиляция оригинальной программы.
Достаточно заново произвести динамическую линковку.
Такой подход очевидно приводит к появлению накладных расходов. 
И расходы тем выше, чем сложнее логика, реализованная на слое профилирования.

В открытом доступе был найден только один готовый MPI профайлер с открытым исходным кодом - mpiP \cite{mpiP}.
При попытке использовать его для решения нашей задачи выяснилось, что формат вывода результатов профилирования плохо структурирован и
было бы достаточно сложно разбирать его программным способом.
К тому же количество функций и возможностей профайлера является избыточном для нашей задачи, 
а значит мы получим накладные расхода на то, что нам не требуется.
Поэтому было принято решение реализовать свой собственной профайлер.

Для решения нашей задачи требуется только вести счетчик количества вызовов MPI функций.
С точки зрения реализации это делается следующим образом.
Для каждой функции из спецификации MPI надо написать обертку, которая будет регистрировать вызов соответствующей MPI функции, 
а затем вызывать оригинальную функцию с теми же аргументами.
Рассмотрим на примере обертки для функции \codebox{MPI_Send}.

\pagebreak
\begin{lstlisting}[language=C, breaklines=true]

MPI_CALLS = malloc(sizeof(int) * MPI_FUNCTIONS_COUNT);
for (int i = 0; i < MPI_FUNCTIONS_COUNT; i++) {
    MPI_CALLS[i] = 0;
}
...
void REGISTER_CALL(int funcId) {
  MPI_CALLS[funcId]++;
}
...
extern int MPI_Send(const void *buf, int count, MPI_Datatype datatype,  int dest, int tag, MPI_Comm comm) {
  REGISTER_CALL(108);
  int res = PMPI_Send(buf, count, datatype, dest, tag, comm);
  return res;
}
\end{lstlisting}

В коде, представленном выше, число 108 это порядковый номер функции \codebox{MPI_Send} в спецификации.
Стоит отметить, что на данный момент в спецификации MPI задекларировано 146 функций, и написание вручную обертки для каждой из них 
заняло много времени и привело бы к большому количеству багов.
Поэтому этот процесс необходимо было автоматизировать и написать алгоритм кодогенерации.
В итоге был разработан Python скрипт, который на основе спецификации генерирует код библиотеки профайлера на языке C.

Последнее требование к проверяющему сервису - возможность независимо оценивать выходные потоки разных MPI процессов.
Утилита \codebox{mpirun}, используемая для запуска MPI программ, создает один общий поток вывода и пишет в него все сообщения, получаемые от разных процессов.
Нет возможности каким-либо образом без изменений кода программы разделить потоки вывода.
Важно отметить, что в ходе реализации предыдущего требования уже был разработан собственный профайлер, который будет использоваться для всех запусков MPI программ.
Таким образом мы можем интегрировать дополнительную логику во все проверяемые решения, так как 
в коде профайлера у нас есть возможность добавлять эту логику в слое интерфейса профилирования.
В том числе в разработанном профайлере уже есть переопределение функции \codebox{MPI_INIT}, 
которая вызывается в любой MPI программе гарантированно раньше вызова всех остальных функций.
Это то место, где есть возможность переопределить поток вывода и сделать это независимо для каждого процесса.
Таким образом можно для каждого процесса настроить перенаправление потока вывода в отдельный файл.

\begin{lstlisting}[language=C, breaklines=true]
static int world_rank = 0;
static int world_size = 0;
...
void REDIRECT_STDOUT() {
    char* fn = (char*)malloc(15);
    sprintf(fn, "stdout_%03d.log", world_rank);
    freopen(fn, "w", stdout);
    free(fn);
}
...
extern int MPI_Init(int *argc, char ***argv) {
	int res = PMPI_Init(argc, argv);
	PMPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
    PMPI_Comm_size(MPI_COMM_WORLD, &world_size);
	REDIRECT_STDOUT();
	REGISTER_CALL(82);
	return res;
}
\end{lstlisting}

После запуска MPI программы с использованием 3 процессов (\codebox{mpirun -n 3 a.out}) получим в текущей директории 3 файла с именами \codebox{stdout_001.log}, \codebox{stdout_002.log}, \codebox{stdout_003.log}.

Реализация MPI профайлера доступна в публичном Github репозитории.\footnote{https://github.com/olegvelikanov/mpi-call-stats}

\subsection{Пример работы системы}

Рассмотрим все особенности работы системы, проследив за конкретным примером проверки решения.
Будем рассматривать базовую задачу типа Hello world, в которой необходимо, чтобы каждый процесс вывел в консоль свой порядковый номер и общее количество процессов.

Взаимодействие студента с системой происходит в интерфейсе LMS.
Студент записан на курсы, каждый из которых поделен на занятия.
В рамках занятия может быть представлено несколько тестовых заданий. 
В рамках этой работы был реализован новый тип тестового задания, который подразумевает отправку решения на проверку в новую систему.
Интерфейс сдачи задания выглядит как форма ввода, в которую студент должен поместить свое решение и нажать кнопку "Закончить попытку" (рис. \ref{fig:LMS_input_form}).


После этого плагин для Moodle, реализующий новый тип тестового задания, делает сетевой HTTP вызов в сервис \codebox{coordinator}.
Продемонстрируем данный запрос в виде curl команды. 
Для краткости опустим тело запроса, которое идет после флага \codebox{--data-raw}. 

\shellcmd{curl -X POST 'http://remote.vdi.mipt.ru:55608/api/submit' \linebreak
  -H 'Accept: application/json, text/plain, */*' \linebreak
  -H 'Content-Type: multipart/form-data; boundary=----WebKitFormBoundaryZ3hOqIr7NY8bn6tu' \linebreak
  -{}-data-raw ...}

Видно, что происходит POST запрос на эндпоинт \codebox{/api/submit}. 
Тип содержимого тела запроса - \codebox{multipart/form-data}.
В теле запроса передается идентификатор задачи, а также решение студента. 
В ответ на такой запрос клиент получает json вида 
\codebox{{"id": "81f98d5d-9adf-40ea-b005-1f5967f3d590"}}, 
который содержит идентификатор посылки.

При обработке такого запроса \codebox{coordinator} создает в реляционной базе данных новую запись с уникальным идентификатором, 
соответствующую новой посылке. 
Далее сервис определяет, что для проверки задачи надо провести серию тестов с разными входными данными и разным числом MPI процессов.
Для каждого теста формируется отдельный zip-архив, содержащий файлы, необходимые для проверки.
В случае с рассматриваемой задачей архив содержит в себе две директории: \codebox{submission} и \codebox{test}.
В директории \codebox{submission} содержится один файл \codebox{submission.c}, в котором находится решение студента.
В директории \codebox{test} хранятся файлы вида \codebox{stdout.001}, которые являются корректным и ожидаемым выводом i-го процесса.
С содержимым этого файла должен сравнивать реальный поток вывода сервис проверки.
Когда все zip-архивы сформированы, каждый из них отправляется отдельным сообщением в очередь \codebox{submit-queue} брокера сообщений.
После того, как все сообщения успешно добавлены в очередь, идентификатор посылки отправляется обратно клиенту, который сделал вызов эндпоинта  \codebox{/api/submit}.

Важно отметить, что сервис \codebox{coordinator} также добавляет к сообщениям заголовки.
Так, например, в одном из заголовков содержится информация о типе проверяемой задачи.
На основе этого заголовка внутри брокера сообщений происходит роутинг, и сообщение из общей очереди попадает в топик, 
на который подписан соответствующий типу задачи проверяющий модуль. 

В случае с задачей MPI Hello world сообщения попадут в топик, который слушает сервис \codebox{mpi-checker}.
Сервис обрабатывает сообщения асинхронно.
На каждое сообщение создается новая корутина, которая должна проверить решение студента в заданной тестовой конфигурации.
Процесс проверки можно разделить на несколько этапов.
В рамках первого этапа происходит подготовка рабочей директории.
Создается временная директория с уникальным названием, содержимое полученного zip-архив разарихивируется в эту директорию, 
затем валидируется ее структура и проверяется наличие всех необходимых для проверки файлов.
На следующем этапе происходит компиляция программы \codebox{submission.c} с нужными флагами и с использованием необходимых библиотек. 
В частности именно на этом этапе происходит линковка MPI профайлера, о котором шла речь в предыдущей главе. 
Также формируется файл запуска slurm задачи  - \codebox{sbatch.sh}.
Затем следует этап запуска и ожидания окончания выполнения.
В рамках него в менеджере slurm регистрируется новая задача, а затем в бесконечном цикле ожидается ее завершение.
Любая задача будет либо успешно завершена, либо остановлена при превышении временных или ресурсных ограничений.
Последний этап - проверка потока вывода.
Читаются файлы вывода процессов и каждый из них сравнивается с ожидаемым. 
Далее анализируются результаты профилировки.
Для каждого процесса проверяется, что не было превышено количество использований MPI функций.
В итоге выносится вердикт.
Сообщение с вердиктом отправляется в очередь \codebox{result-queue} брокера сообщений.

Стоит отметить, что описанный процесс происходит независимо для каждого теста в рамках одной задачи. 
В итоге в очереди результатов будут вердикты по каждому из тестов в отдельности.
Сервис \codebox{coordinator} дождется вердиктов по всем тестам, после чего сможет сделать финальный вердикт по решению студента в целом.
Если хотя бы один из тестов не прошел, студент не получает баллов за задачу. 

Когда финальный вердикт вынесен, он записывается в базу данных и может быть прочитан оттуда в любой момент времени.
Плагин Moodle использует для получения актуального статуса проверки GET запрос, который также можно проиллюстрировать с помощью curl команды.
\shellcmd{curl 'http://localhost:8080/api/status/81f98d5d-9adf-40ea-b005-1f5967f3d590'}

Ответом на такой запрос является json вида 

\begin{lstlisting}[basicstyle=\small, language=json, breaklines=true]
{
    "id": "1f98d5d-9adf-40ea-b005-1f5967f3d590",
    "status": "OK",
    "grade": 1.0,
    "children":
    [
        {
            "id": "4e4b20ff-16c2-471e-9eb0-6869be50fc86",
            "name": "1 process",
            "status": "OK",
            "grade": 1.0,
            "details": {"io": "ok", "time": "331"},
            "options": {"compile_flags": "-lm", "np": 1}
        }, 
        {
            "id": "5e6a0e5b-fc67-4386-bf3e-942b38bb02b4",
            "name": "2 processes",
            "type": "test",
            "status": "OK",
            "grade": 1.0,
            "details": {"io": "ok", "time": "341"},
            "options": {"compile_flags": "-lm","np": 2}
        },
        {
            "id": "91634f64-3321-4491-a2e8-74234b717a79",
            "name": "5 processes",
            "type": "test",
            "status": "OK",
            "grade": 1.0,
            "details": {"io": "ok", "time": "425"},
            "options": {"compile_flags": "-lm","np": 5}
        },
    ]
}
\end{lstlisting}

В нем видно, что было проведено 3 теста с разным числом MPI процессов: 1, 2, 5.
Каждый тест прошел успешно, и итоговый вердикт - максимальный балл. 
Студент видит этот результат в интерфейсе LMS (рис. \ref{fig:LMS_result}).


\begin{figure}[h]
\hspace*{-2.0cm} \includegraphics[scale=0.48]{images/LMS_input_form.png}
\centering
\caption{LMS: форма отправки решения на проверку \label{fig:LMS_input_form}}
\end{figure}

\begin{figure}[h]
\hspace*{-2.0cm} \includegraphics[scale=0.45]{images/LMS_result.png}
\centering
\caption{LMS: результат проверки \label{fig:LMS_result}}
\end{figure}